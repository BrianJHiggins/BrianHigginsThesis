{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8933257,"sourceType":"datasetVersion","datasetId":5374278}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nDescription\nThis notebook takes as input 'Transcripts_Questions_Answers_Test_cleaned_7.csv' and so on\nthe file contains the plain text questions and answers extracted \nfrom Earnings Conference Call transcripts.\nThe file is input to the FinBERT model. \noutput file: 'earnings_calls_Kaggle_Test_q_a.csv'  The output consists of the input returned with\neach question and answer (row) given sentiment classification scores. Each Q and A can be inspected to \ncheck the classification given if required.\nprocessing time is ~ 6 minutes per input file using Kaggle with GPU P100 accelerator.\n\n\"\"\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertForSequenceClassification, BertTokenizer\nimport torch\nimport pandas as pd\nimport csv","metadata":{"execution":{"iopub.status.busy":"2024-07-26T13:23:31.086329Z","iopub.execute_input":"2024-07-26T13:23:31.087107Z","iopub.status.idle":"2024-07-26T13:23:36.818065Z","shell.execute_reply.started":"2024-07-26T13:23:31.087075Z","shell.execute_reply":"2024-07-26T13:23:36.817247Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"\n\nclasses = {0:'positive', 1:'negative', 2:'neutral'}\n\ntokenizer = BertTokenizer.from_pretrained('ProsusAI/finbert')\n\nmodel = BertForSequenceClassification.from_pretrained('ProsusAI/finbert')","metadata":{"execution":{"iopub.status.busy":"2024-07-26T13:23:40.470521Z","iopub.execute_input":"2024-07-26T13:23:40.471191Z","iopub.status.idle":"2024-07-26T13:23:44.752696Z","shell.execute_reply.started":"2024-07-26T13:23:40.471152Z","shell.execute_reply":"2024-07-26T13:23:44.751850Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/252 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec1205158aef48c39d38651773afa3ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8326e8d908444a5ab50ca58fe65afca7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82534063bc2a4beba0dff95b99f25cb2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/758 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbead163335547418d64f9dda8556de6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7f348b88e994bfa830642a8beec5c56"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef text_processing(text):\n    txt = text\n    tokens = tokenizer.encode_plus(txt, add_special_tokens=False)\n    input_ids, token_type_ids, attention_mask = tokens['input_ids'], tokens['token_type_ids'], tokens['attention_mask']\n    total_len = len(tokens['input_ids'])\n    return input_ids, attention_mask, total_len\n    tokens.keys()\n# tokens.keys()    ","metadata":{"execution":{"iopub.status.busy":"2024-07-26T13:23:48.856527Z","iopub.execute_input":"2024-07-26T13:23:48.856909Z","iopub.status.idle":"2024-07-26T13:23:48.862525Z","shell.execute_reply.started":"2024-07-26T13:23:48.856880Z","shell.execute_reply":"2024-07-26T13:23:48.861639Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\ndef chunk_text_to_window_size_and_predict_proba(input_ids, attention_mask, total_len):\n    \"\"\"\n    This function splits the given input text into chunks of a specified window length, \n    applies transformer model to each chunk and computes probabilities of each class for each chunk. \n    The computed probabilities are then appended to a list.\n\n    Args:\n        input_ids (List[int]): List of token ids representing the input text.\n        attention_mask (List[int]): List of attention masks corresponding to input_ids.\n        total_len (int): Total length of the input_ids.\n\n    Returns:\n        proba_list (List[torch.Tensor]): List of probability tensors for each chunk.\n    \"\"\"\n    proba_list = []\n    \n    start = 0\n    window_length = 510\n    \n    loop = True\n    \n    while loop:\n        end = start  + window_length\n        # If the end index exceeds total length, set the flag to False and adjust the end index\n        if end >= total_len:\n            loop = False\n            end = total_len\n\n        # 1 => Define the text chunk\n        input_ids_chunk = input_ids[start : end]\n        attention_mask_chunk = attention_mask[start : end]\n        \n        # 2 => Append [CLS] and [SEP]\n        input_ids_chunk = [101] + input_ids_chunk + [102]\n        attention_mask_chunk = [1] + attention_mask_chunk + [1]\n        \n        #3 Convert regular python list to Pytorch Tensor\n        input_dict = {\n            'input_ids' : torch.Tensor([input_ids_chunk]).long(),\n            'attention_mask' : torch.Tensor([attention_mask_chunk]).int()\n        }\n        \n        outputs = model(**input_dict)\n        probabilities = torch.nn.functional.softmax(outputs[0], dim = -1)\n        proba_list.append(probabilities)\n        start = end\n        \n    return proba_list\n    \n    \n\n# proba_list = chunk_text_to_window_size_and_predict_proba(input_ids, attention_mask, total_len)\n# print(\"This is the 'proba' list:\", proba_list)\n   \n","metadata":{"execution":{"iopub.status.busy":"2024-07-26T13:23:53.274752Z","iopub.execute_input":"2024-07-26T13:23:53.275480Z","iopub.status.idle":"2024-07-26T13:23:53.284533Z","shell.execute_reply.started":"2024-07-26T13:23:53.275447Z","shell.execute_reply":"2024-07-26T13:23:53.283673Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\n\ndef get_mean_from_proba(proba_list):\n    \"\"\"\n    This function computes the mean probabilities of class predictions over all the chunks.\n\n    Args:\n        proba_list (List[torch.Tensor]): List of probability tensors for each chunk.\n\n    Returns:\n        mean (torch.Tensor): Mean of the probabilities across all chunks.\n    \"\"\"\n    \n    # Ensures that gradients are not computed, saving memory\n    with torch.no_grad():\n        # Stack the list of tensors into a single tensor\n        stacks = torch.stack(proba_list)\n\n        # Resize the tensor to match the dimensions needed for mean computation\n        stacks = stacks.resize(stacks.shape[0], stacks.shape[2])\n        # print(\"This is 'stacks':\", stacks) #BH Tue 16Apr 00.27\n\n        # Compute the mean along the zeroth dimension (i.e., the chunk dimension)\n        mean = stacks.mean(dim = 0)\n        \n    return mean\n\n# mean = get_mean_from_proba(proba_list)\n# tensor([0.0767, 0.1188, 0.8045])\n\n# torch.argmax(mean).item()\n# mean\n","metadata":{"execution":{"iopub.status.busy":"2024-07-26T13:24:22.236494Z","iopub.execute_input":"2024-07-26T13:24:22.237106Z","iopub.status.idle":"2024-07-26T13:24:22.243042Z","shell.execute_reply.started":"2024-07-26T13:24:22.237070Z","shell.execute_reply":"2024-07-26T13:24:22.242110Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\noutput_filename = './earnings_calls_Kaggle_Test_q_a.csv'\n\n\n# Note Row 2516 (Index 2515) - had to remove inverted commas: M-Real said there are ` no grounds ' for the rumors , which ` have been circulating in the market for some months . '\n\n\n# df = pd.read_csv('/kaggle/input/d/bjhths/transcripts-questions-and-answers-test-cleaned-csv/Transcripts_Questions_and_Answers_Test_cleaned.csv',encoding='latin1')\ndf = pd.read_csv('/kaggle/input/transcripts-questions-answers-test-cleaned-7-csv/Transcripts_Questions_Answers_Test_cleaned_7.csv',encoding='latin1')\n\nwith open(output_filename, 'w', newline='') as csvfile:\n    csv_writer = csv.writer(csvfile)\n   \n    csv_writer.writerow(['ID','Company_AName','Ticker', 'Text', 'Call_Section', 'Transcript_Text', 'sentiment','count of +ve', 'count of -ve', 'count of neutral'])\n\n    # Iterate over each row in the DataFrame\n    for i, row in df.iterrows():\n        # Extract relevant information from the current row\n        ID = row['ID']\n        Company_AName = row['Company_AName']\n        Ticker = row['Ticker']\n        Text = row['Text']\n        Call_Section = row['Call_Section']\n        Transcript_Text = row['Transcript_Text']\n        # sentiment = row['sentiment']\n\n        # Perform sentiment analysis on the text to get FinBERT sentiment\n        # input_ids, attention_mask, total_len = text_processing(text)\n        \n        # BH 11Apr2024 15.39:- \n        input_ids, attention_mask, total_len = text_processing(Transcript_Text)\n        \n        proba_list = chunk_text_to_window_size_and_predict_proba(input_ids, attention_mask, total_len)\n        mean = get_mean_from_proba(proba_list)\n        result_class = classes[torch.argmax(mean).item()]\n        # print(\"This is the 'proba_list':\",proba_list) # BH Tue 16Apr 20.08\n        # Count of probability classes per line.\n        \n        # Initialize counters\n        count_positive = 0\n        count_negative = 0\n        count_neutral = 0\n        \n        # Count the occurrences of each class\n        for prob in proba_list:\n            pred_class = torch.argmax(prob).item()\n            if pred_class == 0:\n                count_positive += 1\n            elif pred_class == 1:\n                count_negative += 1\n            elif pred_class == 2:\n                count_neutral += 1\n\n        # Write the processed row to the output CSV file\n        csv_writer.writerow([ID, Company_AName, Ticker, Text, Call_Section, Transcript_Text, result_class, count_positive, count_negative, count_neutral]) \n\n        # Write the processed row to the output CSV file\n        # csv_writer.writerow([phrase_id, sentiment, text, result_class])\n        \n        # BH 11Apr2024 15.39:- \n        # Write the processed row to the output CSV file\n        # csv_writer.writerow([ID, Company_AName, Ticker, Text, Call_Section, Transcript_Text, result_class])","metadata":{"execution":{"iopub.status.busy":"2024-07-26T13:24:42.511809Z","iopub.execute_input":"2024-07-26T13:24:42.512720Z","iopub.status.idle":"2024-07-26T13:31:07.714839Z","shell.execute_reply.started":"2024-07-26T13:24:42.512679Z","shell.execute_reply":"2024-07-26T13:31:07.713952Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:836: UserWarning: non-inplace resize is deprecated\n  warnings.warn(\"non-inplace resize is deprecated\")\nToken indices sequence length is longer than the specified maximum sequence length for this model (622 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}