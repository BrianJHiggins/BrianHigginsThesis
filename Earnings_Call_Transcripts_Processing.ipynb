{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "568235b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If required\n",
    "\n",
    "# !pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d540040d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been extracted and written to 'transcripts_list_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "#STEP_1\n",
    "# Webscraping lists of earnings call transcripts id data for later use\n",
    "# Accesses SeekingAlpha API endpoint. Transcripts data presents as list of JSON objects.\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import json\n",
    "\n",
    "from random import randint\n",
    "\n",
    "# Initialize Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Loop through pages\n",
    "# Enter page range. Caution: Limit range to prevent blocking\n",
    "for page_number in range(100,105):\n",
    "\n",
    "    # SeekingAlpha API endpoint\n",
    "    # Construct URL\n",
    "    url = f\"https://seekingalpha.com/api/v3/articles?filter[category]=earnings%3A%3Aearnings-call-transcripts&filter[since]=0&filter[until]=0&include=author%2CprimaryTickers%2CsecondaryTickers&isMounting=true&page[size]=50&page[number]={page_number}\"\n",
    "\n",
    "    # Navigate to the URL\n",
    "    driver.get(url)\n",
    "    time.sleep(randint(4, 9))  # Adjust this delay as needed to ensure the page loads completely\n",
    "\n",
    "    # Extract page content - JSON list\n",
    "    page_content = driver.page_source\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    json_data = json.loads(soup.body.text)\n",
    "\n",
    "    # Extract data and append to CSV\n",
    "    with open('transcripts_list_data.csv', 'a', newline='') as csvfile:\n",
    "        fieldnames = ['id', 'company_name', 'ticker', 'text', 'date']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        for item in json_data['data']:\n",
    "            id = item['id']\n",
    "            title = item['attributes']['title']\n",
    "            company_name, ticker, text = title.split('(')[0].strip(), \\\n",
    "                                          title.split('(')[1].split(')')[0], title.split('(')[1].split(')')[\n",
    "                                              1].strip()\n",
    "            date = item['attributes']['publishOn']\n",
    "\n",
    "            writer.writerow(\n",
    "                {'id': id, 'company_name': company_name, 'ticker': ticker,\n",
    "                 'text': text, 'date': date})\n",
    "\n",
    "    time.sleep(randint(4, 9))\n",
    "\n",
    "print(\"Data has been extracted and written to 'transcripts_list_data.csv'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12e2de3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat GPT Fri 26 Apr 16.25\n",
    "# To achieve scraping in batches with reconnection between each batch,\n",
    "# you can modify your code to close and reopen the Selenium WebDriver after each batch.\n",
    "# Here's the modified version of your code:\n",
    "# Back to line 1323 for the manual version.\n",
    "\n",
    "# 15Jul. this works. A sample file 'transcript_data_test_urls_data.csv'... \n",
    "# ....cut down to 4 rows was used. (directory OS(C)/users/mbjhi)\n",
    "import csv\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from random import randint\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "\n",
    "\n",
    "# Function to scrape text data from a single URL using BeautifulSoup\n",
    "def scrape_text(driver, url):\n",
    "    time.sleep(randint(4, 9))  # Random sleep between 4 and 9 seconds\n",
    "    driver.get(url)\n",
    "    try:\n",
    "        page_content = driver.page_source\n",
    "        soup = BeautifulSoup(page_content, 'html.parser')\n",
    "        text_element = soup.find('body')\n",
    "        if text_element:\n",
    "            transcript_text = text_element.get_text()\n",
    "        else:\n",
    "            transcript_text = None\n",
    "        return transcript_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while scraping text from URL: {url}\")\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "\n",
    "# Function to process a batch of URLs and scrape text data\n",
    "def process_batch(driver, batch, writer):\n",
    "    for row in batch:\n",
    "        id = row['id']\n",
    "        company_name = row['company_name']\n",
    "        ticker = row['ticker']\n",
    "        date = row['date']\n",
    "        text = row['text']\n",
    "\n",
    "        url = f\"https://seekingalpha.com/api/v3/articles/{id}?include=author%2CprimaryTickers%2CsecondaryTickers%2CotherTags%2Cpresentations%2Cpresentations.slides%2Cauthor.authorResearch%2Cauthor.userBioTags%2Cco_authors%2CpromotedService%2Csentiments\"\n",
    "        scraped_text = scrape_text(driver, url)\n",
    "\n",
    "        if scraped_text:\n",
    "            text_chunks = [scraped_text[i:i + 8000] for i in\n",
    "                           range(0, len(scraped_text), 8000)]\n",
    "            for chunk in text_chunks:\n",
    "                writer.writerow(\n",
    "                    {'id': id, 'company_name': company_name, 'ticker': ticker,\n",
    "                     'date': date, 'text': text, 'transcript_text': chunk})\n",
    "        else:\n",
    "            writer.writerow(\n",
    "                {'id': id, 'company_name': company_name, 'ticker': ticker,\n",
    "                 'date': date, 'text': text, 'transcript_text': ''})\n",
    "\n",
    "\n",
    "# Function to process the CSV file and scrape text data\n",
    "def process_csv(input_csv, output_csv):\n",
    "    # Open the WebDriver\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    with open(input_csv, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        with open(output_csv, 'a', newline='', encoding='utf-8') as outfile:\n",
    "            fieldnames = ['id', 'company_name', 'ticker', 'date', 'text',\n",
    "                          'transcript_text']\n",
    "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            batch = []\n",
    "            for row in reader:\n",
    "                batch.append(row)\n",
    "                if len(batch) >= 15:\n",
    "                    process_batch(driver, batch, writer)\n",
    "                    batch = []\n",
    "                    driver.quit()  # Close the WebDriver\n",
    "                    time.sleep(randint(420,\n",
    "                                       540))  # Random sleep between 7 and 9 minutes (420 and 540 seconds)\n",
    "                    driver = webdriver.Chrome()  # Reopen the WebDriver\n",
    "\n",
    "            # Process any remaining URLs\n",
    "            if batch:\n",
    "                process_batch(driver, batch, writer)\n",
    "\n",
    "    # Close the WebDriver\n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "try:\n",
    "    # Process the CSV file and scrape text data\n",
    "    process_csv('transcript_data_test_urls_data.csv',\n",
    "                'transcriptscraped_test_data.csv')\n",
    "except WebDriverException as e:\n",
    "    print(\"Blocked by website\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc7bd364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is df grouped         QTR  Sum_of_Positive  Sum_of_Negative  Sum_of_Neutral\n",
      "0   2021_01                0                0              13\n",
      "1   2021_02                2                0              17\n",
      "2   2021_03              147               19            1053\n",
      "3   2021_04              739              135            5462\n",
      "4   2022_01              819              193            6227\n",
      "5   2022_02              897              273            6197\n",
      "6   2022_03              600              210            4387\n",
      "7   2022_04              831              266            5937\n",
      "8   2023_01              931              275            5987\n",
      "9   2023_02              953              324            6112\n",
      "10  2023_03              844              268            6415\n",
      "11  2023_04              752              190            5260\n",
      "12  2024_01              179               38            1125\n",
      "13  2024_02              130               33             666\n",
      "14  2024_03               36                9             230\n"
     ]
    }
   ],
   "source": [
    "# Test example line 776 in methods.py\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('transcripts_all_analysed_level1_grouped_modified_with_sector_updated.csv')\n",
    "\n",
    "# Filter the DataFrame to include only rows where Call_Section == 'Q&A'\n",
    "df_filtered = df[df['Call_Section'] == 'Q&A Session']\n",
    "# print(df_filtered)\n",
    "\n",
    "\n",
    "# Group by 'QTR' and aggregate the sum of 'Sum_of_Positive' and 'Sum_of_Negative'\n",
    "df_grouped = df_filtered.groupby('QTR')[['Sum_of_Positive', 'Sum_of_Negative', 'Sum_of_Neutral']].sum().reset_index()\n",
    "# df_grouped = df_filtered.groupby('Ticker')[['Sum_of_Positive', 'Sum_of_Negative']].sum().reset_index()\n",
    "print('This is df grouped',df_grouped )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cb237ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Line 685 in methods.py\n",
    "\n",
    "df = pd.read_csv('transcripts_all_analysed_level1_grouped_modified_with_sector_updated.csv')\n",
    "# Aggregate data to count unique IDs per QTR\n",
    "unique_id_counts = df.groupby('QTR')['ID'].nunique().reset_index()\n",
    "unique_id_counts.columns = ['QTR', 'Count of Transcripts']\n",
    "\n",
    "# # Sort QTR in chronological order\n",
    "unique_id_counts = unique_id_counts.sort_values(by='QTR')\n",
    "\n",
    "# Create a bar chart. Uncomment below to show.\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.bar(unique_id_counts['QTR'], unique_id_counts['Count of Transcripts'])\n",
    "# plt.xlabel('QTR')\n",
    "# plt.ylabel('Count of Transcripts')\n",
    "# plt.title('Count of Transcripts per QTR')\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Save the plot to a file\n",
    "# plot_file_path = 'unique_id_counts_per_qtr.png'\n",
    "# plt.savefig('C:/Users/mbjhi/OneDrive/Desktop/BH/Thesis/Data/Production_data/Data_Analysis/unique_id_counts_per_qtr_new.png')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7b8ab84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum_of_positive2 35274\n",
      "sum_of_positive2_byQTR QTR\n",
      "2021_01       6\n",
      "2021_02      11\n",
      "2021_03     712\n",
      "2021_04    3781\n",
      "2022_01    3702\n",
      "2022_02    3938\n",
      "2022_03    2650\n",
      "2022_04    4117\n",
      "2023_01    3712\n",
      "2023_02    3911\n",
      "2023_03    3832\n",
      "2023_04    3524\n",
      "2024_01     745\n",
      "2024_02     463\n",
      "2024_03     170\n",
      "Name: Sum_of_Positive, dtype: int64\n",
      "sum_of_negative2 7758\n",
      "sum_of_negative2_byQTR QTR\n",
      "2021_01       0\n",
      "2021_02       0\n",
      "2021_03      73\n",
      "2021_04     580\n",
      "2022_01     738\n",
      "2022_02     851\n",
      "2022_03     656\n",
      "2022_04     931\n",
      "2023_01     880\n",
      "2023_02    1006\n",
      "2023_03     964\n",
      "2023_04     771\n",
      "2024_01     165\n",
      "2024_02     106\n",
      "2024_03      37\n",
      "Name: Sum_of_Negative, dtype: int64\n",
      "This is tone 0.639431121026213\n"
     ]
    }
   ],
   "source": [
    "# How to drop rows where there is no entry in a particular coulmn in that row\n",
    "# https://www.aporia.com/resources/how-to/drop-rows-pandas-dataframe-column-vamue-nan/\n",
    "df2 = df.dropna(subset=[\"QTR\"])\n",
    "sum_of_positive2 = df2['Sum_of_Positive'].sum()\n",
    "print('sum_of_positive2', sum_of_positive2)\n",
    "sum_of_positive2_byQTR = df2.groupby('QTR')['Sum_of_Positive'].sum()\n",
    "print('sum_of_positive2_byQTR', sum_of_positive2_byQTR)\n",
    "\n",
    "sum_of_negative2 = df2['Sum_of_Negative'].sum()\n",
    "print('sum_of_negative2', sum_of_negative2)\n",
    "sum_of_negative2_byQTR = df2.groupby('QTR')['Sum_of_Negative'].sum()\n",
    "print('sum_of_negative2_byQTR', sum_of_negative2_byQTR)\n",
    "tone = (sum_of_positive2 - sum_of_negative2) / (sum_of_positive2 + sum_of_negative2)\n",
    "print('This is tone',tone)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafd5338",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Qs and As STEP 1\n",
    "# ChatGPT  Q and A extraction problem Sun 08 Jul  09.36  Use this one. Note:This is the original code with an additional new function at the end\n",
    "# Third version with headings added This works (the earlier two versions worked also but\n",
    "# No1 wrote the output to the new file in plain text and no headings on either output file\n",
    "# No2 wrote the output correctly in HTML format to the new file, but no headings on either output file\n",
    "# See the ChatGPT word document!\n",
    "# back to line 2140 (this code goes to 2669)\n",
    "# This version below has headers added and works good\n",
    "# This accesses the raw transcripts and extracts Qs and As separately\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "\n",
    "# Function to extract text between tags or provide notification if not found\n",
    "def extract_text(data, start_tag, alt_start_tag, end_tag, alt_end_tag):\n",
    "    start_index = data.find(start_tag)\n",
    "    if start_index == -1:\n",
    "        start_index = data.find(alt_start_tag)\n",
    "        if start_index == -1:\n",
    "            return \"Tag not found\"\n",
    "\n",
    "    end_index = data.find(end_tag)\n",
    "    if end_index == -1:\n",
    "        end_index = data.find(alt_end_tag)\n",
    "        if end_index == -1:\n",
    "            return \"Tag not found\"\n",
    "\n",
    "    text = data[start_index:end_index]\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    return soup.get_text(strip=True)\n",
    "\n",
    "\n",
    "# Function to extract raw HTML between tags or provide notification if not found\n",
    "def extract_html(data, start_tag, alt_start_tag, end_tag, alt_end_tag):\n",
    "    start_index = data.find(start_tag)\n",
    "    if start_index == -1:\n",
    "        start_index = data.find(alt_start_tag)\n",
    "        if start_index == -1:\n",
    "            return \"Tag not found\"\n",
    "\n",
    "    end_index = data.find(end_tag)\n",
    "    if end_index == -1:\n",
    "        end_index = data.find(alt_end_tag)\n",
    "        if end_index == -1:\n",
    "            return \"Tag not found\"\n",
    "\n",
    "    return data[start_index:end_index + len(end_tag)]\n",
    "\n",
    "\n",
    "# Define the chunk size\n",
    "chunk_size = 8000\n",
    "\n",
    "\n",
    "# Function to split text into chunks\n",
    "def split_text_into_chunks(text):\n",
    "    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# Read CSV file\n",
    "# input_csv = 'transcriptscraped_test_data_testing.csv'\n",
    "#BH Sun 07JUl\n",
    "#Part1 below\n",
    "# input_csv = 'transcriptscraped_test_data_master_Part1_12Apr.csv'\n",
    "# input_csv = 'transcriptscraped_test_data_Sat03May_1930 to 2230_Part6_B.csv'\n",
    "input_csv = 'transcriptscraped_test_data_master_Part1_12Apr.csv'\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Output file names\n",
    "output_file = 'transcripts.csv'\n",
    "output_qa_file = 'transcripts_Qs_and_As.csv'\n",
    "\n",
    "# Remove output files if they exist (to avoid appending to old data during testing)\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "if os.path.exists(output_qa_file):\n",
    "    os.remove(output_qa_file)\n",
    "\n",
    "# Iterate over unique IDs\n",
    "for id_val in df['id'].unique():\n",
    "    # Filter data for the current ID\n",
    "    df_id = df[df['id'] == id_val]\n",
    "\n",
    "    # Get values\n",
    "    company_name_val = df_id['company_name'].iloc[0]\n",
    "    ticker_val = df_id['ticker'].iloc[0]\n",
    "    date_val = df_id['date'].iloc[0]\n",
    "    text_val = df_id['text'].iloc[0]\n",
    "    transcript_text = df_id['transcript_text'].str.cat(\n",
    "        sep=' ')  # Concatenate all transcript texts\n",
    "\n",
    "    # Extract company statement text\n",
    "    company_statement = extract_text(transcript_text,\n",
    "                                     '<strong>Company Participants</strong>',\n",
    "                                     '<strong>Corporate Participants</strong>',\n",
    "                                     '<strong>Question-and-Answer Session</strong>',\n",
    "                                     '<strong>Question-and-Answer Session</strong>')\n",
    "\n",
    "    # Extract Q&A session text\n",
    "    q_and_a = extract_text(transcript_text,\n",
    "                           '<strong>Question-and-Answer Session</strong>',\n",
    "                           '<strong>Question-and-Answer Session</strong>',\n",
    "                           'twitContent',\n",
    "                           'twitContent')\n",
    "\n",
    "    # Split texts into chunks\n",
    "    company_chunks = split_text_into_chunks(company_statement)\n",
    "    qa_chunks = split_text_into_chunks(q_and_a)\n",
    "\n",
    "    # Create DataFrame for company statement\n",
    "    df_company = pd.DataFrame({'ID': id_val,\n",
    "                               'Company Name': company_name_val,\n",
    "                               'Ticker': ticker_val,\n",
    "                               'Date': date_val,\n",
    "                               'text': text_val,\n",
    "                               'call_section': 'company_statement',\n",
    "                               'transcript_text': company_chunks})\n",
    "\n",
    "    # Create DataFrame for Q&A session\n",
    "    df_qa = pd.DataFrame({'ID': id_val,\n",
    "                          'Company Name': company_name_val,\n",
    "                          'Ticker': ticker_val,\n",
    "                          'Date': date_val,\n",
    "                          'text': text_val,\n",
    "                          'call_section': 'Q&A Session',\n",
    "                          'transcript_text': qa_chunks})\n",
    "\n",
    "    # Concatenate DataFrames\n",
    "    df_concatenated = pd.concat([df_company, df_qa], ignore_index=True)\n",
    "\n",
    "    # Write DataFrame to CSV with header\n",
    "    if not os.path.exists(output_file):\n",
    "        df_concatenated.to_csv(output_file, mode='w', index=False, header=True)\n",
    "    else:\n",
    "        df_concatenated.to_csv(output_file, mode='a', index=False,\n",
    "                               header=False)\n",
    "\n",
    "\n",
    "# New function to write Q&A data to a separate CSV file\n",
    "def write_q_and_a_html_to_csv():\n",
    "    # Read the same input CSV file again\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    all_q_and_a_html = []\n",
    "\n",
    "    for id_val in df['id'].unique():\n",
    "        # Filter data for the current ID\n",
    "        df_id = df[df['id'] == id_val]\n",
    "\n",
    "        # Get values\n",
    "        company_name_val = df_id['company_name'].iloc[0]\n",
    "        ticker_val = df_id['ticker'].iloc[0]\n",
    "        date_val = df_id['date'].iloc[0]\n",
    "        text_val = df_id['text'].iloc[0]\n",
    "        transcript_text = df_id['transcript_text'].str.cat(\n",
    "            sep=' ')  # Concatenate all transcript texts\n",
    "\n",
    "        # Extract Q&A session HTML\n",
    "        q_and_a_html = extract_html(transcript_text,\n",
    "                                    '<strong>Question-and-Answer Session</strong>',\n",
    "                                    '<strong>Question-and-Answer Session</strong>',\n",
    "                                    'twitContent',\n",
    "                                    'twitContent')\n",
    "\n",
    "        # Split texts into chunks\n",
    "        qa_chunks = split_text_into_chunks(q_and_a_html)\n",
    "\n",
    "        # Create DataFrame for Q&A session HTML\n",
    "        df_qa_html = pd.DataFrame({'ID': id_val,\n",
    "                                   'Company Name': company_name_val,\n",
    "                                   'Ticker': ticker_val,\n",
    "                                   'Date': date_val,\n",
    "                                   'text': text_val,\n",
    "                                   'call_section': 'Q&A Session',\n",
    "                                   'transcript_text': qa_chunks})\n",
    "\n",
    "        # Collect all Q&A data\n",
    "        all_q_and_a_html.append(df_qa_html)\n",
    "\n",
    "    # Concatenate all Q&A data and write to a separate CSV with header\n",
    "    if all_q_and_a_html:\n",
    "        df_all_q_and_a_html = pd.concat(all_q_and_a_html, ignore_index=True)\n",
    "        if not os.path.exists(output_qa_file):\n",
    "            df_all_q_and_a_html.to_csv(output_qa_file, mode='w', index=False,\n",
    "                                       header=True)\n",
    "        else:\n",
    "            df_all_q_and_a_html.to_csv(output_qa_file, mode='a', index=False,\n",
    "                                       header=False)\n",
    "\n",
    "\n",
    "# Call the new function to write Q&A data to the new CSV file\n",
    "write_q_and_a_html_to_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92978df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Qs and As STEP 2\n",
    "# ChatGPT remove specific text and lines from Q-A file Fri 05Jul 23.45\n",
    "# go to line 2479 (this code goes to 2263)\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Load the CSV files into DataFrames\n",
    "# df = pd.read_csv('transcriptscraped_test_data_testing.csv')\n",
    "# df = pd.read_csv('C:/Users/mbjhi/PycharmProjects/Wk1Assign/Transcripts_Q_A/Part1/transcriptscraped_test_data_master_Part1_12Apr.csv')\n",
    "# df = pd.read_csv('C:/Users/mbjhi/PycharmProjects/Wk1Assign/Transcripts_Q_A/Part1/transcriptscraped_test_data_testing.csv')\n",
    "df = pd.read_csv('transcripts_Qs_and_As.csv')\n",
    "additional_data = pd.read_csv('transcripts_all_analysed_level1_grouped_modified_with_sector_updated.csv')\n",
    "\n",
    "# Combine text for each transcript ID\n",
    "combined_texts = df.groupby('id')['transcript_text'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "# Function to clean text  (BH This is the original that works just not perfectly!)\n",
    "def clean_text(text):\n",
    "    # Remove 'good morning' or 'good evening' regardless of case\n",
    "    #text = re.sub(r'(?i)good morning|good evening', '', text)\n",
    "    # BH\n",
    "    #text = re.sub(r'(?i)good morning|good evening|good afternoon|good question|great question|Thanks for taking the question|Thanks for the question|Thank you|Thanks|Great', '', text)\n",
    "    text = re.sub(r'(?i)good morning|good evening|good afternoon|good question|great question|'\n",
    "                  r'Thanks for taking the question|Thanks for the question|Thank you|Thanks|Great', '', text)\n",
    "    return text\n",
    "\n",
    "# Function to extract and clean questions and answers\n",
    "def extract_questions_answers(html_content):\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Initialize variables to store questions and answers\n",
    "    qa_list = []\n",
    "    current_question = None\n",
    "    current_answer = None\n",
    "\n",
    "    # Iterate through paragraphs to find questions and answers\n",
    "    for p in soup.find_all('p'):\n",
    "        strong_tag = p.find('strong')\n",
    "        if strong_tag:\n",
    "            span_tag = strong_tag.find('span')\n",
    "            if span_tag:\n",
    "                span_class = span_tag.get('class')\n",
    "                if span_class and 'question' in span_class[0]:\n",
    "                    if current_question and current_answer:\n",
    "                        qa_list.append(('Q', clean_text(current_question)))\n",
    "                        qa_list.append(('A', clean_text(current_answer)))\n",
    "                        current_answer = None\n",
    "                    current_question = p.get_text(strip=True)\n",
    "                elif span_class and 'answer' in span_class[0]:\n",
    "                    current_answer = p.get_text(strip=True)\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            if current_answer:\n",
    "                current_answer += \" \" + p.get_text(strip=True)\n",
    "            elif current_question:\n",
    "                current_question += \" \" + p.get_text(strip=True)\n",
    "\n",
    "    # Append the last question-answer pair if any\n",
    "    if current_question and current_answer:\n",
    "        qa_list.append(('Q', clean_text(current_question)))\n",
    "        qa_list.append(('A', clean_text(current_answer)))\n",
    "\n",
    "    return qa_list\n",
    "\n",
    "\n",
    "# Initialize list to store the extracted data\n",
    "extracted_data = []\n",
    "\n",
    "# Process each combined text\n",
    "for _, row in combined_texts.iterrows():\n",
    "    transcript_id = row['id']\n",
    "    html_content = row['transcript_text']\n",
    "    print(f\"Processing ID: {transcript_id}\")\n",
    "    print(f\"HTML Content: {html_content[:500]}...\")  # Print the first 500 characters of the HTML content for debugging\n",
    "\n",
    "    qa_list = extract_questions_answers(html_content)\n",
    "    if not qa_list:\n",
    "        print(f\"No Q&A pairs found for ID: {transcript_id}\")\n",
    "\n",
    "    for q_a, text in qa_list:\n",
    "        # Filter out lines containing 'next question' and lines with fewer than ten words\n",
    "        if 'next question' in text.lower() or len(text.split()) < 10:\n",
    "            continue\n",
    "        print(f\"{q_a}: {text}\")  # Print the question/answer for debugging\n",
    "        extracted_data.append({\n",
    "            'id': transcript_id,\n",
    "            'Q/A': q_a,\n",
    "            'Q_A_text': text\n",
    "        })\n",
    "\n",
    "# Convert the extracted data to a DataFrame\n",
    "extracted_df = pd.DataFrame(extracted_data)\n",
    "\n",
    "# Print columns of additional_data to verify column names\n",
    "print(\"Columns in additional_data:\", additional_data.columns)\n",
    "\n",
    "# Rename 'ID' column in additional_data to 'id' for consistency\n",
    "additional_data.rename(columns={'ID': 'id'}, inplace=True)\n",
    "\n",
    "# Check for the existence of 'id' column\n",
    "if 'id' not in additional_data.columns:\n",
    "    raise KeyError(\"'id' column not found in additional_data\")\n",
    "\n",
    "# Define the columns to add\n",
    "columns_to_add = ['Company_Name', 'Ticker', 'GICS Sector', 'Text', 'QUARTER', 'QTR', 'day_date_formatted']\n",
    "\n",
    "# Check if all columns to add exist in additional_data\n",
    "missing_columns = [col for col in columns_to_add if col not in additional_data.columns]\n",
    "if missing_columns:\n",
    "    raise KeyError(f\"Columns {missing_columns} not found in additional_data\")\n",
    "\n",
    "# Aggregate additional_data to ensure one row per 'id'\n",
    "aggregated_additional_data = additional_data.groupby('id')[columns_to_add].first().reset_index()\n",
    "\n",
    "# Merge with the additional data based on 'id' column\n",
    "merged_df = extracted_df.merge(aggregated_additional_data, on='id', how='left')\n",
    "\n",
    "# Reorder columns to place the new columns after 'id'\n",
    "columns_order = ['id'] + columns_to_add + ['Q/A', 'Q_A_text']\n",
    "merged_df = merged_df[columns_order]\n",
    "\n",
    "# Write the merged data to a new CSV file\n",
    "merged_df.to_csv('Transcripts_Qs_and_As_part1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ec33b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files unzipped to: FinBERT output files combined\n"
     ]
    }
   ],
   "source": [
    "# python\n",
    "# Copy code\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Define the path to the uploaded zip file (assuming it's in the same directory as the notebook)\n",
    "zip_file_path = 'FinBERT output files combined.zip'\n",
    "# Define the directory to unzip files (same directory as the notebook)\n",
    "unzip_folder_path = 'FinBERT output files combined'\n",
    "\n",
    "# Create a directory to unzip the files into\n",
    "os.makedirs(unzip_folder_path, exist_ok=True)\n",
    "\n",
    "# Unzip the file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(unzip_folder_path)\n",
    "\n",
    "print(f\"Files unzipped to: {unzip_folder_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a23bb602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: FinBERT output files combined\\FinBERT output files combined\\earnings_calls_Kaggle_part1.csv\n",
      "Processing file: FinBERT output files combined\\FinBERT output files combined\\earnings_calls_Kaggle_part10.csv\n",
      "Processing file: FinBERT output files combined\\FinBERT output files combined\\earnings_calls_Kaggle_part2.csv\n",
      "Processing file: FinBERT output files combined\\FinBERT output files combined\\earnings_calls_Kaggle_part3.csv\n",
      "Processing file: FinBERT output files combined\\FinBERT output files combined\\earnings_calls_Kaggle_part4.csv\n",
      "Processing file: FinBERT output files combined\\FinBERT output files combined\\earnings_calls_Kaggle_part5A.csv\n",
      "Processing file: FinBERT output files combined\\FinBERT output files combined\\earnings_calls_Kaggle_part5B.csv\n",
      "Processing file: FinBERT output files combined\\FinBERT output files combined\\earnings_calls_Kaggle_part6A.csv\n",
      "Processing file: FinBERT output files combined\\FinBERT output files combined\\earnings_calls_Kaggle_part6B.csv\n",
      "Processing file: FinBERT output files combined\\FinBERT output files combined\\earnings_calls_Kaggle_part6C.csv\n",
      "Processing file: FinBERT output files combined\\FinBERT output files combined\\earnings_calls_Kaggle_part7A.csv\n",
      "Processing file: FinBERT output files combined\\FinBERT output files combined\\earnings_calls_Kaggle_part7B.csv\n",
      "Processing file: FinBERT output files combined\\FinBERT output files combined\\earnings_calls_Kaggle_part8A.csv\n",
      "Processing file: FinBERT output files combined\\FinBERT output files combined\\earnings_calls_Kaggle_part8B.csv\n",
      "Processing file: FinBERT output files combined\\FinBERT output files combined\\earnings_calls_Kaggle_part9A.csv\n",
      "Processing file: FinBERT output files combined\\FinBERT output files combined\\earnings_calls_Kaggle_part9B.csv\n",
      "Grouping and combination of CSV files is complete. Output saved to 'Qs_and_As_grouped.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Qs and As Step 3\n",
    "\n",
    "# ChatGPT Transcripts Qs and As combine and group individual files Fri 12Jul 0659\n",
    "\n",
    "# Mon 15Jul This is line 326 MGMT.py\n",
    "# Update to place output in same folder as py.charm and change name of fields\n",
    "# Pg8\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the directory containing the CSV files (in the same location as the script)\n",
    "directory = 'FinBERT output files combined'\n",
    "\n",
    "# Define the fields to group by\n",
    "group_fields = ['ID', 'Call_Section']\n",
    "\n",
    "# Define the fields to sum\n",
    "sum_fields = ['count of +ve', 'count of -ve', 'count of neutral']\n",
    "\n",
    "# Define the fields to keep\n",
    "keep_fields = ['Company_AName', 'Ticker', 'Text']\n",
    "\n",
    "# Initialize an empty list to store the grouped DataFrames\n",
    "grouped_dataframes = []\n",
    "\n",
    "# Walk through the directory to find all CSV files\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for filename in files:\n",
    "        if filename.endswith('.csv'):\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(root, filename)\n",
    "            # print(f\"Processing file: {file_path}\")\n",
    "\n",
    "            try:\n",
    "                # Read the CSV file into a DataFrame\n",
    "                df = pd.read_csv(file_path)\n",
    "\n",
    "                # Ensure the sum fields are numeric\n",
    "                for field in sum_fields:\n",
    "                    df[field] = pd.to_numeric(df[field], errors='coerce')\n",
    "\n",
    "                # Select the necessary fields\n",
    "                df = df[group_fields + sum_fields + keep_fields]\n",
    "\n",
    "                # Group the DataFrame by the specified fields and sum the required fields\n",
    "                grouped_df = df.groupby(group_fields + keep_fields)[sum_fields].sum().reset_index()\n",
    "\n",
    "                # Append the grouped DataFrame to the list\n",
    "                grouped_dataframes.append(grouped_df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "# Check if any DataFrames were processed\n",
    "if not grouped_dataframes:\n",
    "    print(\"No valid CSV files were processed.\")\n",
    "else:\n",
    "    # Concatenate all the grouped DataFrames\n",
    "    combined_df = pd.concat(grouped_dataframes)\n",
    "\n",
    "    # Group the combined DataFrame again to ensure proper aggregation\n",
    "    combined_df = combined_df.groupby(group_fields + keep_fields)[sum_fields].sum().reset_index()\n",
    "\n",
    "    # Rename the columns as required\n",
    "    combined_df = combined_df.rename(columns={\n",
    "        'count of +ve': 'sum of +ve',\n",
    "        'count of -ve': 'sum of -ve',\n",
    "        'count of neutral': 'sum of neutral'\n",
    "    })\n",
    "\n",
    "    # Define the output file path\n",
    "    output_file_path = 'Qs_and_As_grouped.csv'\n",
    "\n",
    "    # Save the combined DataFrame to a new CSV file in the current working directory\n",
    "    combined_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"Grouping and combination of CSV files is complete. Output saved to '{output_file_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43be3a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated file saved as updated_Qs_and_As_grouped.csv\n"
     ]
    }
   ],
   "source": [
    "# Qs and As Step 4\n",
    "\n",
    "# ChatGPT new columns and update ‘GCIS_Sector’ ‘QTR’  and ‘day_date’ in file ‘combined.csv’ Fri 12 Jul\n",
    "# Line 393 in MGMT.py\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "file_path = 'Qs_and_As_grouped.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define the new columns with default values or empty strings\n",
    "df['GICS Sector'] = ''  # You can replace '' with default values if needed\n",
    "df['QTR'] = ''          # You can replace '' with default values if needed\n",
    "df['day_date'] = ''     # You can replace '' with default values if needed\n",
    "#BH added tone,price_chng_5day,price_chng_2day,price_chng_1day Fri 12 Jul\n",
    "df['tone'] = ''     # You can replace '' with default values if needed\n",
    "df['price_chng_5day'] = ''     # You can replace '' with default values if needed\n",
    "df['price_chng_2day'] = ''     # You can replace '' with default values if needed\n",
    "df['price_chng_1day'] = ''     # You can replace '' with default values if needed\n",
    "\n",
    "\n",
    "# Reorder columns to place the new columns between 'Call_Section' and 'Company_Aname'\n",
    "# columns = ['ID', 'Call_Section', 'GICS Sector', 'QTR', 'day_date', 'Company_AName', 'Ticker', 'Text', 'sum of +ve', 'sum of -ve', 'sum of neutral', 'tone', 'price_chng_5day','price_chng_2day', 'price_chng_1day' ]\n",
    "# df = df[columns]\n",
    "#BH 15 Jul\n",
    "columns = ['ID', 'Call_Section', 'GICS Sector', 'QTR', 'day_date', 'Company_AName', 'Ticker', 'Text', 'sum of +ve', 'sum of -ve', 'sum of neutral', 'tone', 'price_chng_5day','price_chng_2day', 'price_chng_1day' ]\n",
    "df = df[columns]\n",
    "\n",
    "\n",
    "\n",
    "# sum of +ve\tsum of -ve\tsum of neutral\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "output_file_path = 'updated_Qs_and_As_grouped.csv'\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Updated file saved as {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6f74f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Qs and As Step 5\n",
    "\n",
    "#ChatGPT Update new columns GICS Sector, Day_Date, QTR in file combined 12Jul20.28\n",
    "# Line 430 in MGMT.py\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files into DataFrames\n",
    "df_combined = pd.read_csv('updated_Qs_and_As_grouped.csv')\n",
    "df_transcripts = pd.read_csv('transcripts_all_analysed_level1_grouped_modified_with_sector_updated.csv')\n",
    "\n",
    "# Convert the 'ID' columns to strings to ensure they match in type\n",
    "df_combined['ID'] = df_combined['ID'].astype(str)\n",
    "df_transcripts['ID'] = df_transcripts['ID'].astype(str)\n",
    "\n",
    "# Group the transcripts DataFrame by 'ID'\n",
    "grouped_transcripts = df_transcripts.groupby('ID')\n",
    "\n",
    "# Function to update row with new data if available\n",
    "def update_row(row):\n",
    "    if row['ID'] in grouped_transcripts.groups:\n",
    "        # Get the group for this ID\n",
    "        group = grouped_transcripts.get_group(row['ID'])\n",
    "        # Use the first occurrence of each value for simplicity\n",
    "        row['GICS Sector'] = group['GICS Sector'].values[0]\n",
    "        row['day_date'] = group['day_date_formatted'].values[0]\n",
    "        row['QTR'] = group['QTR'].values[0]\n",
    "    return row\n",
    "\n",
    "# Apply the update_row function to each row in the combined DataFrame\n",
    "df_combined = df_combined.apply(update_row, axis=1)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df_combined.to_csv('updated_Qs_and_As_grouped_updated.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3697ecb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
